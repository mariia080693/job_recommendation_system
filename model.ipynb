{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa59946",
   "metadata": {},
   "source": [
    "# Job Recommendation System\n",
    "\n",
    "\n",
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load job ads and queries from JSONL files\n",
    "job_ads = [json.loads(line) for line in open(\"ad_detail_v1.jsonl\", \"r\")]\n",
    "job_ads_df = pd.DataFrame(job_ads)\n",
    "\n",
    "queries = [json.loads(line) for line in open(\"qry_rel_v1.jsonl\", \"r\")]\n",
    "queries_df = pd.DataFrame(queries)\n",
    "\n",
    "print(f\"Job ads: {len(job_ads_df)} Queries: {len(queries_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82baf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the mean and standard deviation of relevance counts per query\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relevance_levels = [4, 3, 2, 1]\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for rel in relevance_levels:\n",
    "    counts = queries_df['relevance'].apply(lambda x: x.count(rel))\n",
    "    means.append(counts.mean())\n",
    "    stds.append(counts.std())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(relevance_levels, means, yerr=stds, capsize=8,color=\"#3b5348\", edgecolor='black')\n",
    "plt.xlabel('Relevance')\n",
    "plt.ylabel('Average Count per Query')\n",
    "plt.title('Mean and Std of Relevance Counts per Query')\n",
    "plt.xticks(relevance_levels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdab00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total relevance score per query\n",
    "total_relevance_per_query = queries_df['relevance'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(total_relevance_per_query, bins=20, color=\"#3b5348\", edgecolor='black')\n",
    "plt.xlabel('Job count per Query')\n",
    "plt.ylabel('Number of Queries')\n",
    "plt.title('Job count per Query')\n",
    "\n",
    "# Set custom x-ticks with smaller delta\n",
    "min_x = total_relevance_per_query.min()\n",
    "max_x = total_relevance_per_query.max()\n",
    "plt.xticks(np.arange(min_x, max_x + 1, step=50))  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac804fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique keys in metadata\n",
    "unique_keys = {key for data in job_ads_df['metadata'] if isinstance(data, dict) for key in data}\n",
    "unique_keys       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cffda7",
   "metadata": {},
   "source": [
    "\n",
    "## Combining Preprocessed Job Data for Vectorization\n",
    "#### (preprocessing inclused cleaning, normalization, preservation of special terms, and stemming ). Salary data are excluded for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f475b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_text import clean_text\n",
    "\n",
    "# Combine all cleaned text fields into one column\n",
    "job_ads_df['clean_jobs'] = (\n",
    "    job_ads_df['title'].apply(clean_text) + ' ' + \n",
    "    job_ads_df['abstract'].apply(clean_text) + ' ' + \n",
    "    job_ads_df['content'].apply(clean_text) + ' ' +\n",
    "    job_ads_df['metadata'].apply(clean_text)\n",
    ").apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Process search queries\n",
    "queries_df['clean_query'] = queries_df['query_keywords'].apply(clean_text)\n",
    "\n",
    "print(f\"updated job_ads_df: {list(job_ads_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ads_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80664b00",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cfe22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_job_matrix = tfidf_vectorizer.fit_transform(job_ads_df['clean_jobs'])\n",
    "tfidf_query_matrix = tfidf_vectorizer.transform(queries_df['clean_query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df['clean_query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b31ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_job_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb64ac7",
   "metadata": {},
   "source": [
    "# Data balancing\n",
    "#### (undersampling using the smallest ad_ids count across queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ad_ids_len = queries_df['ad_ids'].apply(len).min()\n",
    "print(f\"Min ad_ids length: {min_ad_ids_len}\")\n",
    "\n",
    "max_ad_ids_len = queries_df['ad_ids'].apply(len).max()\n",
    "print(f\"Max ad_ids length: {max_ad_ids_len}\")\n",
    "\n",
    "mean_ad_ids_len = queries_df['ad_ids'].apply(len).mean()\n",
    "print(f\"Mean ad_ids length: {mean_ad_ids_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling\n",
    "queries_df['ad_ids_balanced'] = queries_df['ad_ids'].apply(lambda x: x[:min_ad_ids_len])\n",
    "queries_df['relevance_balanced'] = queries_df['relevance'].apply(lambda x: x[:min_ad_ids_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6894355",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df.head()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a92da0",
   "metadata": {},
   "source": [
    "# Adding zeros\n",
    "#### (Job ads not mentioned in queries should be treated as having relevance '0'. We add 5 such jobs with '0' relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unlabeled jobs with zero relevance\n",
    "import random\n",
    "\n",
    "unique_job_ads = set(job_ads_df['ad_id'])\n",
    "    \n",
    "def add_zero_relevance(row):\n",
    "    miss_ad_ids = list(unique_job_ads - set(row['ad_ids']))\n",
    "    if miss_ad_ids:\n",
    "        sampled_ids = random.sample(miss_ad_ids, min_ad_ids_len)\n",
    "        row['ad_ids_balanced'] = list(row['ad_ids_balanced']) + sampled_ids\n",
    "        row['relevance_balanced'] = list(row['relevance_balanced']) + [0] * min_ad_ids_len\n",
    "    return row\n",
    "\n",
    "queries_df = queries_df.apply(add_zero_relevance, axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a3fd4",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "print(queries_df['ad_ids_balanced'].apply(lambda x: x[-10:]))\n",
    "print(queries_df['relevance_balanced'].apply(lambda x: x[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0266e",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_queries_df, test_queries_df = train_test_split(queries_df, test_size=0.2, random_state=42)\n",
    "print(f\"Train queries: {len(train_queries_df)}, Test queries: {len(test_queries_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f79e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02396f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ads_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962641ed",
   "metadata": {},
   "source": [
    "# Creating query-job pairs for training and test data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1985d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_pair_df(sample_queries_df, job_ads_df):\n",
    "    data = []\n",
    "    \n",
    "    for query_idx, query_row in sample_queries_df.iterrows():\n",
    "        #clean_query = query_row['clean_query']\n",
    "        ad_ids_balanced = query_row['ad_ids_balanced']\n",
    "        relevance_balanced = query_row['relevance_balanced']\n",
    "        \n",
    "        for ad_id, relevance in zip(ad_ids_balanced, relevance_balanced):\n",
    "            job_ad = job_ads_df[job_ads_df['ad_id'] == ad_id]  \n",
    "            if not job_ad.empty:\n",
    "                job_idx = job_ad.index[0]\n",
    "                data.append({\n",
    "                    'query_idx': query_idx,\n",
    "                    'job_id': ad_id,\n",
    "                    'job_idx': job_idx,\n",
    "                    'relevance': relevance,\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "training_data_df = build_pair_df(train_queries_df, job_ads_df)\n",
    "test_data_df = build_pair_df(test_queries_df, job_ads_df)\n",
    "\n",
    "print(f\"Training_labeled_df:\", list(training_data_df.columns))\n",
    "print(f\"Number of query-job pairs: {len(training_data_df)}\")\n",
    "print(f\"\\nRelevance distribution: {training_data_df['relevance'].value_counts().sort_index(ascending = False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daea7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156065ee",
   "metadata": {},
   "source": [
    "# Building feature matrix and labels array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "def build_feature_matrix(sample_data_df):\n",
    "    X_features = []\n",
    "    y_labels = []\n",
    "    for i, row in sample_data_df.iterrows():\n",
    "        job_idx = row['job_idx']\n",
    "        query_idx = row['query_idx']\n",
    "        job_vec = tfidf_job_matrix[job_idx]\n",
    "        query_vec = tfidf_query_matrix[query_idx]\n",
    "        \n",
    "        # Combine job and query features\n",
    "        pair_vec = hstack([job_vec, query_vec])\n",
    "        \n",
    "        # Append the feature vectors and labels\n",
    "        X_features.append(pair_vec)\n",
    "        y_labels.append(row['relevance'])\n",
    "\n",
    "    # Stack the feature vectors and convert labels to numpy array\n",
    "    X = vstack(X_features)\n",
    "    y = np.array(y_labels)\n",
    "        \n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = build_feature_matrix(training_data_df)\n",
    "X_test, y_test = build_feature_matrix(test_data_df)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b42e1",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94718562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "xgb_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    verbosity=2,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict relevance\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbf4fe",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, ndcg_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2 = r2_score(y_train, y_train_pred)\n",
    "mae = mean_absolute_error(y_train, y_train_pred)\n",
    "ndcg = ndcg_score([y_train], [y_train_pred])\n",
    "\n",
    "print(f\"Train RMSE: {rmse:.4f}\")\n",
    "print(f\"Train R^2: {r2:.4f}\")\n",
    "print(f\"Train MAE: {mae:.4f}\")\n",
    "print(f\"Train NDCG: {ndcg:.4f}\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mae = mean_absolute_error(y_test, y_test_pred)\n",
    "ndcg = ndcg_score([y_test], [y_test_pred])\n",
    "\n",
    "print(f\"\\nTest RMSE: {rmse:.4f}\")\n",
    "print(f\"Test R^2: {r2:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test NDCG: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa5c51",
   "metadata": {},
   "source": [
    "# Saving Model and Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777df3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Save XGBoost model\n",
    "xgb_model.save_model(\"model/xgb_model.json\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(tfidf_vectorizer, \"model/tfidf_vectorizer.joblib\")\n",
    "\n",
    "# Save TF-IDF job matrix\n",
    "joblib.dump(tfidf_job_matrix, \"model/tfidf_job_matrix.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
